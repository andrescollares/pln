{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4azOYi8KSoC"
      },
      "source": [
        "# Laboratorio de Introducci√≥n al Procesamiento de Lenguaje Natural\n",
        "\n",
        "### Contexto\n",
        "\n",
        "El objetivo de este laboratorio es introducirlos a la construcci√≥n de clasificadores, probando y comparando diferentes m√©todos.\n",
        "\n",
        "### Entrega\n",
        "Lo que deber√°n entregar es un archivo *.ipynb* con su soluci√≥n, que incluya c√≥digo, discusiones y conclusiones del trabajo. \n",
        "\n",
        "‚ö†Ô∏è Es importante que en el archivo a entregar est√©n **las salidas de cada celda ya ejecutadas** ‚ö†Ô∏è. \n",
        "\n",
        "En caso de hacer el ejercicio 3, opcional, deber√°n entregar tambi√©n un archivo .csv **correctamente formateado** con las predicciones de sus modelos.\n",
        "\n",
        "El plazo m√°ximo es el **21 de octubre a las 23:59 horas.**\n",
        "\n",
        "### Plataforma sugerida\n",
        "Sugerimos que utilicen la plataforma [Google colab](https://colab.research.google.com/), que permite trabajar colaborativamente con un *notebook* de python. Al finalizar pueden descargar ese *notebook* en un archivo .ipynb, incluyendo las salidas ya ejecutadas, con la opci√≥n ```File -> Download -> Download .ipynb```\n",
        "\n",
        "### Instalaci√≥n de bibliotecas\n",
        "Antes de empezar, ejecuten esta celda para instalar las dependencias üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pzvAUVqGdGW7",
        "outputId": "d81fa012-ae81-4595-d98a-1f72bae4288f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/andres/fing/nsql/lab2/.venv/bin/python\n",
            "/home/andres/.local/bin/pip\n"
          ]
        }
      ],
      "source": [
        "!which python\n",
        "!which pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_7FpsJWGszpa",
        "outputId": "df26db7a-346a-4a4c-94ea-dede165cf3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from sklearn) (1.1.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.22.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.2.0)\n",
            "Requirement already satisfied: gensim in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (1.22.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (1.9.2)\n",
            "Requirement already satisfied: spacy in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (3.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.22.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (8.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: setuptools in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
            "Requirement already satisfied: jinja2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
            "Requirement already satisfied: nltk in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (2022.9.13)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install sklearn\n",
        "!python -m pip install gensim\n",
        "!python -m pip install spacy\n",
        "!python -m pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAumcYFLP0f8"
      },
      "source": [
        "# Ejercicio 1 - Primer contacto con el corpus\n",
        "\n",
        "Lo primero a hacer es cargar el corpus. Hay muchas formas de hacerlo ([por ejemplo con Pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)), pero la m√°s sencilla es utilizando funcionalidades nativas de python. El resultado ser√° una lista de n-uplas, donde cada una de ellas se correpondes a una fila del .csv (incluso el cabezal, la primera l√≠nea).\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î \n",
        "Carguen a Colab los archivos necesarios del corpus usando el panel de la izquierda y luego ejecuten las siguientes celdas. Ajusten lo necesario para cargar todo el conjunto de *train* (`train_1.csv` y `train_2.csv`), el de dev y tambi√©n sus anotaciones, que van a ser un subconjunto del archivo `train_1.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nnBJGtH5QLcA"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "\"\"\"\n",
        "  Completen con su c√≥digo de carga de archivos \"train\" y \"dev\" ac√°\n",
        "\"\"\"\n",
        "\n",
        "with open('tweets_grupo_D_1.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  original_annotations = [x for x in reader]\n",
        "\n",
        "with open('train_1.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  train_set = [x for x in reader]\n",
        "\n",
        "with open('train_2.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  train_set = train_set + [x for x in reader] \n",
        "\n",
        "with open('dev.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  dev_set = [x for x in reader]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5vw6mqiNbgD"
      },
      "source": [
        "Imprimamos alg√∫n tweet aleatorio a ver si se carg√≥ bien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldfWPEtCNebS",
        "outputId": "1e51865b-83f1-4765-bb42-10b6186037ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El tweet es: Ofensivamente el Liverpool es un maldito Hurac√°n, un gol en cualquier momento puede caer. #ChampionsLeague\n",
            "y su categor√≠a: others\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Elegir un tweet aleatorio e imprimirlo junto a su categor√≠a\n",
        "random_tweet = random.choice(dev_set)\n",
        "print(f\"El tweet es: {random_tweet[1]}\")\n",
        "print(f\"y su categor√≠a: {random_tweet[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HFf4BwD36uZ"
      },
      "source": [
        "## Parte 1.1 - Composici√≥n de los conjuntos de entrenamiento y desarrollo\n",
        "\n",
        "Para ver c√≥mo esta compuesto el corpus, van a hacer una recorrida sobre todos los tweets en √©l y contar cu√°ntos ejemplos hay de cada categor√≠a. Examinen, discutan y comparen la cantidad de ejemplos en cada categor√≠a, en *train* y en *dev* ¬øhay m√°s ejemplos de unas categor√≠as que de otras? ¬øtienen la misma proporci√≥n en *train* y *dev*?\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Recorran los conjuntos, saquen conclusiones y escr√≠banlas en una celda de texto, a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNBy_qybD_oY",
        "outputId": "a3335c91-de12-4e2b-eee6-320d0d41fb41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set length: 5723\n",
            "Dev set length: 1250\n",
            "Train set tweet amount by category:\n",
            "{   'anger': 563,\n",
            "    'disgust': 103,\n",
            "    'fear': 63,\n",
            "    'joy': 1230,\n",
            "    'others': 2786,\n",
            "    'sadness': 733,\n",
            "    'surprise': 245}\n",
            "Dev set tweet amount by category:\n",
            "{   'anger': 150,\n",
            "    'disgust': 27,\n",
            "    'fear': 14,\n",
            "    'joy': 259,\n",
            "    'others': 617,\n",
            "    'sadness': 131,\n",
            "    'surprise': 52}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "print(f\"Train set length: {len(train_set)}\")\n",
        "print(f\"Dev set length: {len(dev_set)}\")\n",
        "\n",
        "# Contar cu√°ntos ejemplos hay de cada catergor√≠a\n",
        "# Train\n",
        "train_set_categories = [tweet[2] for tweet in train_set]\n",
        "\n",
        "category_names = set(train_set_categories)\n",
        "\n",
        "train_set_categories_count = dict.fromkeys(category_names, 0)\n",
        "\n",
        "for category in train_set_categories:\n",
        "    train_set_categories_count[category] += 1\n",
        "\n",
        "print(\"Train set tweet amount by category:\")\n",
        "pp.pprint(train_set_categories_count)\n",
        "\n",
        "# Dev\n",
        "dev_set_categories = [tweet[2] for tweet in dev_set]\n",
        "\n",
        "category_names = set(dev_set_categories)\n",
        "\n",
        "dev_set_categories_count = dict.fromkeys(category_names, 0)\n",
        "\n",
        "for category in dev_set_categories:\n",
        "    dev_set_categories_count[category] += 1\n",
        "\n",
        "print(\"Dev set tweet amount by category:\")\n",
        "pp.pprint(dev_set_categories_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDuZTO6udGXD"
      },
      "source": [
        "Aunque hay mas ejemplos de una que de otras, esto es esperable dado que el tama√±o del conjunto de \"\"\"**Dev**\"\"\" (no se como se llama en realidad) es m√°s peque√±o que el de train. Lo importante es que la distribuci√≥n de los datos sea similar y como se puede observar en el resultado de la celda de c√≥digo, esto se cumple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPrpyYhcdGXD",
        "outputId": "012a4468-e5e2-4234-edc7-e472dc0b1811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set distribution\n",
            "{   'anger': 0.1,\n",
            "    'disgust': 0.02,\n",
            "    'fear': 0.01,\n",
            "    'joy': 0.21,\n",
            "    'others': 0.49,\n",
            "    'sadness': 0.13,\n",
            "    'surprise': 0.04}\n",
            "Dev set distribution\n",
            "{   'anger': 0.12,\n",
            "    'disgust': 0.02,\n",
            "    'fear': 0.01,\n",
            "    'joy': 0.21,\n",
            "    'others': 0.49,\n",
            "    'sadness': 0.1,\n",
            "    'surprise': 0.04}\n"
          ]
        }
      ],
      "source": [
        "print(\"Train set distribution\")\n",
        "pp.pprint({item[0]:round(item[1]/len(train_set),2) for item in train_set_categories_count.items()})\n",
        "print(\"Dev set distribution\")\n",
        "pp.pprint({item[0]:round(item[1]/len(dev_set),2) for item in dev_set_categories_count.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVPMAnvaJkXA"
      },
      "source": [
        "## Parte 1.2 - C√°lculo del acuerdo entre anotadores\n",
        "\n",
        "A continuaci√≥n queremos ver cu√°n de acuerdo estuvieron grupalmente con las anotaciones originales. Para eso deber√°n usar [esta funci√≥n disponible en sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score).\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î  Calculen el grado de acuerdo entre las antoaciones originales y las suyas como grupo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GIwwIOilECnH",
        "outputId": "55c2da61-1e49-404d-f054-d49be9ad8937"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.401386263390044"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "original_annotations_categories = [annotation[2] for annotation in original_annotations]\n",
        "train_set_categories = [annotation[2] for annotation in train_set[600:800]]\n",
        "\n",
        "cohen_kappa_score(original_annotations_categories, train_set_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJfW75x5dGXE"
      },
      "source": [
        "TODO: Explicar un poco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRniq2WASeTt"
      },
      "source": [
        "# Ejercicio 2 - Experimentos con clasificadores\n",
        "\n",
        "Ahora que cargaron y examinaron los datos, van a crear un primer clasificador para resolver el problema autom√°ticamente. Como los clasificadores asumen que sus atributos son num√©ricos, hay que encontrar primero una forma num√©rica de representar los textos. En este ejercicio van a experimentar con varias formas de hacer eso.\n",
        "\n",
        "En todas las partes podr√°n usar cualquiera de los clasificadores disponibles en el cat√°logo de modelos de [aprendizaje supervisado de sklearn](https://scikit-learn.org/stable/supervised_learning.html).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZMlbsw2uFLm"
      },
      "source": [
        "## Parte 2.1 - Bag of Words\n",
        "\n",
        "El primer experimento es utilizando Bag of Words (BoW) para representar los textos. Ac√° les dejamos un ejemplo, pero prueben con las diferentes configuraciones que admite [CountVectorizer de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) y con los modelos que quieran del [cat√°logo de sklearn](https://scikit-learn.org/stable/supervised_learning.html). Tambi√©n pueden explorar diferentes formas de limpieza de los textos. \n",
        "\n",
        "Midan el aprendizaje sobre *dev* con la m√©trica [$F_1$, con la implementaci√≥n de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). Tambi√©n pueden usar otras m√©tricas adicionales; queda a su disposici√≥n.\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Hagan varios experimentos con diferentes tipos de clasificadores y diferentes configuraciones de BoW para vectorizar. Midan el aprendizajen con $F_1$. Discutan, reflexionen y escriban las conclusiones en una celda de texto a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9BNltLduHqB",
        "outputId": "25bb54aa-515e-4db0-e164-38b63214b263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag of Words:\n",
            "Classifier       Accuracy    Precision    Recall    F-Score\n",
            "multinomial      61.44       52.35        30.00     30.30\n",
            "decision_tree    54.96       39.90        37.40     38.41\n",
            "neighbours       54.40       30.79        22.62     22.62\n",
            "mlpc_classifier  60.32       50.76        41.52     44.39\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Completen con su c√≥digo ac√°\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "\n",
        "# Posibles parametros a tocar: Strip accents, lowercase\n",
        "bow_vectorizer = CountVectorizer()  # Vectorizador \"bag of words\"\n",
        "\n",
        "classifiers = [\n",
        "    [\"multinomial\", MultinomialNB()],\n",
        "    [\"decision_tree\", DecisionTreeClassifier(random_state=0)],\n",
        "    [\"neighbours\", KNeighborsClassifier(n_neighbors=10)],\n",
        "    [\"mlpc_classifier\", MLPClassifier(random_state=0, max_iter=100)],\n",
        "]\n",
        "\n",
        "print(\"Bag of Words:\")\n",
        "print(\"Classifier       Accuracy    Precision    Recall    F-Score\")\n",
        "for clf in classifiers:\n",
        "    training_features = bow_vectorizer.fit_transform(\n",
        "        [x[1] for x in train_set]\n",
        "    )  # Se vectorizan los tweets de train\n",
        "\n",
        "    clf[1].fit(\n",
        "        training_features, [x[2] for x in train_set]\n",
        "    )  # Se entrena el clasificador usando los tweets vectorizados\n",
        "\n",
        "    dev_features = bow_vectorizer.transform(\n",
        "        [x[1] for x in dev_set]\n",
        "    )  # Se vectorizan los tweets de dev\n",
        "    prediction = clf[1].predict(\n",
        "        dev_features\n",
        "    )  # Se predicen las categor√≠as de cada tweet (ya vectorizado en la l√≠nea anterior)\n",
        "\n",
        "    accuracy = \"{:.2f}\".format(round(accuracy_score([x[2] for x in dev_set], prediction) * 100, 2))\n",
        "    precision = \"{:.2f}\".format(round(precision_score([x[2] for x in dev_set], prediction, average=\"macro\", zero_division=0) * 100, 2))\n",
        "    recall = \"{:.2f}\".format(round(recall_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2))\n",
        "    fscore = \"{:.2f}\".format(round(f1_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2))\n",
        "\n",
        "    print(clf[0] + ' '*(17-len(clf[0])) + accuracy + ' '*7 + precision + ' '*8 + recall + ' '*5 + fscore)  # Se imprime la medida F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuVYSnU7wdKA"
      },
      "source": [
        "Con los F-score obtenidos para los distintos clasificadores, entendemos que para los par√°metros dados el clasificador MultiLayerPerception supera a los dem√°s en clasificar el dev_set correctamente utilizando la vectorizaci√≥n Bag of Words. Analizando tambi√©n los valores de Accuracy, Precision y Recall, entendemos que el MultinomialNB fue m√°s acertivo en encontrar verdaderos positivos, pero resulta en un Recall menor al de MLP por obtener m√°s falsos negativos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74o3beG-_tCq"
      },
      "source": [
        "## Parte 2.2 - TF-iDF\n",
        "\n",
        "El segundo es utilizando TF-iDF (Term Frequency - inverse Document Frequency) para representar los textos.\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Lo an√°logo a la parte anterior pero probando diferentes configuraciones con [TF-iDF de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Comparen los resultados de $F_1$ con los obtenidos para Bag of Words y escriban las conclusiones en una celda de texto a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vgTQJXL_D42e",
        "outputId": "6e1c3052-910a-4e19-b376-19114daf5123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F-Score multinomial: 14.11\n",
            "F-Score decision_tree: 38.69\n",
            "F-Score neighbours: 38.99\n",
            "F-Score mlpc_classifier: 44.15\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Posibles parametros a tocar: Strip accents, lowercase\n",
        "tfid_vectorizer = TfidfVectorizer()\n",
        "  # Vectorizador \"Term Frequency - inverse Document Frequency\"\n",
        "\n",
        "classifiers = [\n",
        "    [\"multinomial\", MultinomialNB()],\n",
        "    [\"decision_tree\", DecisionTreeClassifier(random_state=0)],\n",
        "    [\"neighbours\", KNeighborsClassifier(n_neighbors=10)],\n",
        "    [\"mlpc_classifier\", MLPClassifier(random_state=0, max_iter=100)],\n",
        "]\n",
        "\n",
        "for clf in classifiers:\n",
        "    training_features = tfid_vectorizer.fit_transform(\n",
        "        [x[1] for x in train_set]\n",
        "    )  # Se vectorizan los tweets de train\n",
        "    clf[1].fit(\n",
        "        training_features, [x[2] for x in train_set]\n",
        "    )  # Se entrena el clasificador usando los tweets vectorizados\n",
        "\n",
        "    dev_features = tfid_vectorizer.transform(\n",
        "        [x[1] for x in dev_set]\n",
        "    )  # Se vectorizan los tweets de dev\n",
        "    prediction = clf[1].predict(\n",
        "        dev_features\n",
        "    )  # Se predicen las categor√≠as de cada tweet (ya vectorizado en la l√≠nea anterior)\n",
        "\n",
        "    print(\n",
        "        f\"F-Score {clf[0]}: \"\n",
        "        + str(\n",
        "            round(\n",
        "                f1_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2\n",
        "            )\n",
        "        )\n",
        "    )  # Se imprime la medida F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIPSS06GDV-J"
      },
      "source": [
        "## Parte 2.3 - Word embeddings\n",
        "\n",
        "El tercer y √∫ltimo enfoque es utilizando **vectores de palabras est√°ticos** para representar los textos. Hay much√≠simas colecciones de vectores de palabras, pero en esta ocasi√≥n vamos a usar unos entrenados por la Univerisdad de Chile. \n",
        "\n",
        "Una idea simple pero √∫til para representar un tweet puede ser hallar el centroide de los vectores relacionados a las palabras que aparecen en √©l, y luego comparar cu√°l es m√°s similar a cu√°l. \n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Lo an√°logo a las partes anteriores pero probando con una representaci√≥n basada en *embeddings*. Comparen con $F_1$, saquen conclusiones y escr√≠banlas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIg11w8rGCAN"
      },
      "source": [
        "\n",
        "Les dejamos el siguiente c√≥digo de ejemplo. Permite cargar los vectores, hallar el centroide de una lista de tokens y calcular las similitudes entre diferentes centroides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se descargan los vectores\n",
        "# !wget -q http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.100k.vec.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qK0kyukPdGXH",
        "outputId": "4ec3c8d3-30f8-4e40-e8c8-c1542bd50ea9"
      },
      "outputs": [],
      "source": [
        "# !gzip -d -q fasttext-sbwc.100k.vec.gz\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6MlhUYqidBvR",
        "outputId": "2c1a18c0-039d-4c72-ba7d-da4aa81ff8c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages/gensim/matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
            "  from scipy.linalg.special_matrices import triu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0000001]]\n",
            "[[0.90079546]]\n",
            "[[0.78608865]]\n",
            "[[0.7549316]]\n",
            "[[0.5619315]]\n",
            "[[0.70001805]]\n"
          ]
        }
      ],
      "source": [
        "from numpy.linalg import norm\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Se crea el objeto\n",
        "vectors = KeyedVectors.load_word2vec_format('fasttext-sbwc.100k.vec')\n",
        "\n",
        "# Unos ejemplos, ya tokenizados\n",
        "example_1 = [\"Qu√©\", \"tremendo\", \"d√≠a\", \"hace\"]\n",
        "example_2 = [\"Qu√©\", \"lindo\", \"d√≠a\", \"hace\"]\n",
        "example_3 = [\"Hace\", \"un\", \"precioso\", \"d√≠a\"]\n",
        "example_4 = [\"Hoy\", \"est√°\", \"lindo\", \"el\", \"d√≠a\"]\n",
        "example_5 = [\"Pah\", \"esta\", \"milanesa\", \"con\", \"mayonesa\", \"est√°\", \"buen√≠sima\"]\n",
        "example_6 = [\"Ya\", \"le\", \"dije\", \"que\", \"le\", \"vas\", \"a\", \"escribir\"]\n",
        "\n",
        "# Se calculan los centroides\n",
        "centroid_example_1 = np.mean([vectors[word.lower()] for word in example_1], axis=0)\n",
        "centroid_example_2 = np.mean([vectors[word.lower()] for word in example_2], axis=0)\n",
        "centroid_example_3 = np.mean([vectors[word.lower()] for word in example_3], axis=0)\n",
        "centroid_example_4 = np.mean([vectors[word.lower()] for word in example_4], axis=0)\n",
        "centroid_example_5 = np.mean([vectors[word.lower()] for word in example_5], axis=0)\n",
        "centroid_example_6 = np.mean([vectors[word.lower()] for word in example_6], axis=0)\n",
        "\n",
        "# Se imprime la similitud entre los centroides del ejemplo 1 y el resto. \n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_1]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_2]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_3]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_4]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_5]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xS-TBgmIEU3R",
        "outputId": "2c4e00b6-b09c-49e8-f188-93b4ec8bb365"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.33596590774266616"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.utils import tokenize\n",
        "from statistics import mode\n",
        "\n",
        "# Se calculan todos los centroides de test y dev\n",
        "def get_centroid(vectors, tweet):\n",
        "    tokenized_tweet = list(tokenize(tweet, lowercase=True))\n",
        "    v = []\n",
        "    for word in tokenized_tweet:\n",
        "      try:\n",
        "        v.append(vectors[word])\n",
        "      except KeyError:\n",
        "        pass\n",
        "    return np.mean(v, axis=0)\n",
        "\n",
        "\n",
        "train_centroids = np.array([get_centroid(vectors, x[1]) for x in train_set], dtype=float)\n",
        "\n",
        "dev_centroids = np.array([get_centroid(vectors, x[1]) for x in dev_set], dtype=float)\n",
        "\n",
        "classifications = []\n",
        "# Se calcula la similitud de cada centroide de dev a todos los centroides de test\n",
        "dev_similarities = cosine_similarity(dev_centroids, train_centroids)\n",
        "# Se consiguen los 10 indices mas similares\n",
        "ind_most_similar = [np.argpartition(dev_similarity, -10)[-10:] for dev_similarity in dev_similarities]\n",
        "# Se elige la emocion mas frecuente como clasificaci√≥n\n",
        "classifications = [(mode([train_set[i][2] for i in index])) for index in ind_most_similar]\n",
        "\n",
        "F1 = f1_score([x[2] for x in dev_set], classifications, average=\"macro\")\n",
        "\n",
        "F1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0uSp9-JH4EN"
      },
      "source": [
        "# Ejercicio 3 (opcional)\n",
        "\n",
        "Esta √∫ltima parte es **opcional**. Ahora que vieron c√≥mo crear clasificadores, invitamos a que intenten construir el mejor clasificador posible utilizando estos enfoques o cualquier otro. Pueden probar lo que quieran, desde enfoques por reglas, utilizando POS-tagging, an√°lisis sint√°ctico, an√°lisis morfol√≥gico o listas de palabras, a modelos neuronales como BERT.\n",
        "\n",
        "Si realizan esta parte opcional, tendr√°n que entregar en EVA las predicciones para un archivo de *test* que subiremos pr√≥ximo a la entrega. Los grupos que obtengan las 3 mejores medidas al evaluar en el conjunto de test ganar√°n 5 puntos porcentuales que sumar√°n para la nota final del curso.\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Construir el mejor clasificador posible y subir a EVA las predicciones para *test*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bi2k9V9FD63d",
        "outputId": "36d52a42-39e0-4f44-c996-b92848f185c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n  Completen con su c√≥digo ac√°\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Completen con su c√≥digo ac√°\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
