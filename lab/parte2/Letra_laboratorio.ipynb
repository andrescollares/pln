{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4azOYi8KSoC"
      },
      "source": [
        "# Laboratorio de Introducción al Procesamiento de Lenguaje Natural\n",
        "\n",
        "### Contexto\n",
        "\n",
        "El objetivo de este laboratorio es introducirlos a la construcción de clasificadores, probando y comparando diferentes métodos.\n",
        "\n",
        "### Entrega\n",
        "Lo que deberán entregar es un archivo *.ipynb* con su solución, que incluya código, discusiones y conclusiones del trabajo. \n",
        "\n",
        "⚠️ Es importante que en el archivo a entregar estén **las salidas de cada celda ya ejecutadas** ⚠️. \n",
        "\n",
        "En caso de hacer el ejercicio 3, opcional, deberán entregar también un archivo .csv **correctamente formateado** con las predicciones de sus modelos.\n",
        "\n",
        "El plazo máximo es el **21 de octubre a las 23:59 horas.**\n",
        "\n",
        "### Plataforma sugerida\n",
        "Sugerimos que utilicen la plataforma [Google colab](https://colab.research.google.com/), que permite trabajar colaborativamente con un *notebook* de python. Al finalizar pueden descargar ese *notebook* en un archivo .ipynb, incluyendo las salidas ya ejecutadas, con la opción ```File -> Download -> Download .ipynb```\n",
        "\n",
        "### Instalación de bibliotecas\n",
        "Antes de empezar, ejecuten esta celda para instalar las dependencias 👇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pzvAUVqGdGW7",
        "outputId": "d81fa012-ae81-4595-d98a-1f72bae4288f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/andres/fing/nsql/lab2/.venv/bin/python\n",
            "/home/andres/.local/bin/pip\n"
          ]
        }
      ],
      "source": [
        "!which python\n",
        "!which pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_7FpsJWGszpa",
        "outputId": "df26db7a-346a-4a4c-94ea-dede165cf3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from sklearn) (1.1.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.22.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.2.0)\n",
            "Requirement already satisfied: gensim in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (1.22.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (1.9.2)\n",
            "Requirement already satisfied: spacy in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (3.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.22.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (8.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: setuptools in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
            "Requirement already satisfied: jinja2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
            "Requirement already satisfied: nltk in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (2022.9.13)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install sklearn\n",
        "!python -m pip install gensim\n",
        "!python -m pip install spacy\n",
        "!python -m pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAumcYFLP0f8"
      },
      "source": [
        "# Ejercicio 1 - Primer contacto con el corpus\n",
        "\n",
        "Lo primero a hacer es cargar el corpus. Hay muchas formas de hacerlo ([por ejemplo con Pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)), pero la más sencilla es utilizando funcionalidades nativas de python. El resultado será una lista de n-uplas, donde cada una de ellas se correpondes a una fila del .csv (incluso el cabezal, la primera línea).\n",
        "\n",
        "🧐**¿Qué tienen que hacer?**🤔 \n",
        "Carguen a Colab los archivos necesarios del corpus usando el panel de la izquierda y luego ejecuten las siguientes celdas. Ajusten lo necesario para cargar todo el conjunto de *train* (`train_1.csv` y `train_2.csv`), el de dev y también sus anotaciones, que van a ser un subconjunto del archivo `train_1.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nnBJGtH5QLcA"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "\"\"\"\n",
        "  Completen con su código de carga de archivos \"train\" y \"dev\" acá\n",
        "\"\"\"\n",
        "\n",
        "with open('tweets_grupo_D_1.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  original_annotations = [x for x in reader]\n",
        "\n",
        "with open('train_1.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  train_set = [x for x in reader]\n",
        "\n",
        "with open('train_2.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  train_set = train_set + [x for x in reader] \n",
        "\n",
        "with open('dev.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  dev_set = [x for x in reader]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5vw6mqiNbgD"
      },
      "source": [
        "Imprimamos algún tweet aleatorio a ver si se cargó bien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldfWPEtCNebS",
        "outputId": "1e51865b-83f1-4765-bb42-10b6186037ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El tweet es: Ofensivamente el Liverpool es un maldito Huracán, un gol en cualquier momento puede caer. #ChampionsLeague\n",
            "y su categoría: others\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Elegir un tweet aleatorio e imprimirlo junto a su categoría\n",
        "random_tweet = random.choice(dev_set)\n",
        "print(f\"El tweet es: {random_tweet[1]}\")\n",
        "print(f\"y su categoría: {random_tweet[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HFf4BwD36uZ"
      },
      "source": [
        "## Parte 1.1 - Composición de los conjuntos de entrenamiento y desarrollo\n",
        "\n",
        "Para ver cómo esta compuesto el corpus, van a hacer una recorrida sobre todos los tweets en él y contar cuántos ejemplos hay de cada categoría. Examinen, discutan y comparen la cantidad de ejemplos en cada categoría, en *train* y en *dev* ¿hay más ejemplos de unas categorías que de otras? ¿tienen la misma proporción en *train* y *dev*?\n",
        "\n",
        "🧐**¿Qué tienen que hacer?**🤔 Recorran los conjuntos, saquen conclusiones y escríbanlas en una celda de texto, a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNBy_qybD_oY",
        "outputId": "a3335c91-de12-4e2b-eee6-320d0d41fb41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set length: 5723\n",
            "Dev set length: 1250\n",
            "Train set tweet amount by category:\n",
            "{   'anger': 563,\n",
            "    'disgust': 103,\n",
            "    'fear': 63,\n",
            "    'joy': 1230,\n",
            "    'others': 2786,\n",
            "    'sadness': 733,\n",
            "    'surprise': 245}\n",
            "Dev set tweet amount by category:\n",
            "{   'anger': 150,\n",
            "    'disgust': 27,\n",
            "    'fear': 14,\n",
            "    'joy': 259,\n",
            "    'others': 617,\n",
            "    'sadness': 131,\n",
            "    'surprise': 52}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "print(f\"Train set length: {len(train_set)}\")\n",
        "print(f\"Dev set length: {len(dev_set)}\")\n",
        "\n",
        "# Contar cuántos ejemplos hay de cada catergoría\n",
        "# Train\n",
        "train_set_categories = [tweet[2] for tweet in train_set]\n",
        "\n",
        "category_names = set(train_set_categories)\n",
        "\n",
        "train_set_categories_count = dict.fromkeys(category_names, 0)\n",
        "\n",
        "for category in train_set_categories:\n",
        "    train_set_categories_count[category] += 1\n",
        "\n",
        "print(\"Train set tweet amount by category:\")\n",
        "pp.pprint(train_set_categories_count)\n",
        "\n",
        "# Dev\n",
        "dev_set_categories = [tweet[2] for tweet in dev_set]\n",
        "\n",
        "category_names = set(dev_set_categories)\n",
        "\n",
        "dev_set_categories_count = dict.fromkeys(category_names, 0)\n",
        "\n",
        "for category in dev_set_categories:\n",
        "    dev_set_categories_count[category] += 1\n",
        "\n",
        "print(\"Dev set tweet amount by category:\")\n",
        "pp.pprint(dev_set_categories_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDuZTO6udGXD"
      },
      "source": [
        "Aunque hay mas ejemplos de una que de otras, esto es esperable dado que el tamaño del conjunto de \"\"\"**Dev**\"\"\" (no se como se llama en realidad) es más pequeño que el de train. Lo importante es que la distribución de los datos sea similar y como se puede observar en el resultado de la celda de código, esto se cumple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPrpyYhcdGXD",
        "outputId": "012a4468-e5e2-4234-edc7-e472dc0b1811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set distribution\n",
            "{   'anger': 0.1,\n",
            "    'disgust': 0.02,\n",
            "    'fear': 0.01,\n",
            "    'joy': 0.21,\n",
            "    'others': 0.49,\n",
            "    'sadness': 0.13,\n",
            "    'surprise': 0.04}\n",
            "Dev set distribution\n",
            "{   'anger': 0.12,\n",
            "    'disgust': 0.02,\n",
            "    'fear': 0.01,\n",
            "    'joy': 0.21,\n",
            "    'others': 0.49,\n",
            "    'sadness': 0.1,\n",
            "    'surprise': 0.04}\n"
          ]
        }
      ],
      "source": [
        "print(\"Train set distribution\")\n",
        "pp.pprint({item[0]:round(item[1]/len(train_set),2) for item in train_set_categories_count.items()})\n",
        "print(\"Dev set distribution\")\n",
        "pp.pprint({item[0]:round(item[1]/len(dev_set),2) for item in dev_set_categories_count.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVPMAnvaJkXA"
      },
      "source": [
        "## Parte 1.2 - Cálculo del acuerdo entre anotadores\n",
        "\n",
        "A continuación queremos ver cuán de acuerdo estuvieron grupalmente con las anotaciones originales. Para eso deberán usar [esta función disponible en sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score).\n",
        "\n",
        "🧐**¿Qué tienen que hacer?**🤔  Calculen el grado de acuerdo entre las antoaciones originales y las suyas como grupo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GIwwIOilECnH",
        "outputId": "55c2da61-1e49-404d-f054-d49be9ad8937"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.401386263390044"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "original_annotations_categories = [annotation[2] for annotation in original_annotations]\n",
        "train_set_categories = [annotation[2] for annotation in train_set[600:800]]\n",
        "\n",
        "cohen_kappa_score(original_annotations_categories, train_set_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJfW75x5dGXE"
      },
      "source": [
        "TODO: Explicar un poco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRniq2WASeTt"
      },
      "source": [
        "# Ejercicio 2 - Experimentos con clasificadores\n",
        "\n",
        "Ahora que cargaron y examinaron los datos, van a crear un primer clasificador para resolver el problema automáticamente. Como los clasificadores asumen que sus atributos son numéricos, hay que encontrar primero una forma numérica de representar los textos. En este ejercicio van a experimentar con varias formas de hacer eso.\n",
        "\n",
        "En todas las partes podrán usar cualquiera de los clasificadores disponibles en el catálogo de modelos de [aprendizaje supervisado de sklearn](https://scikit-learn.org/stable/supervised_learning.html).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZMlbsw2uFLm"
      },
      "source": [
        "## Parte 2.1 - Bag of Words\n",
        "\n",
        "El primer experimento es utilizando Bag of Words (BoW) para representar los textos. Acá les dejamos un ejemplo, pero prueben con las diferentes configuraciones que admite [CountVectorizer de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) y con los modelos que quieran del [catálogo de sklearn](https://scikit-learn.org/stable/supervised_learning.html). También pueden explorar diferentes formas de limpieza de los textos. \n",
        "\n",
        "Midan el aprendizaje sobre *dev* con la métrica [$F_1$, con la implementación de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). También pueden usar otras métricas adicionales; queda a su disposición.\n",
        "\n",
        "🧐**¿Qué tienen que hacer?**🤔 Hagan varios experimentos con diferentes tipos de clasificadores y diferentes configuraciones de BoW para vectorizar. Midan el aprendizajen con $F_1$. Discutan, reflexionen y escriban las conclusiones en una celda de texto a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9BNltLduHqB",
        "outputId": "25bb54aa-515e-4db0-e164-38b63214b263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag of Words:\n",
            "Classifier       Accuracy    Precision    Recall    F-Score\n",
            "multinomial      61.44       52.35        30.00     30.30\n",
            "decision_tree    54.96       39.90        37.40     38.41\n",
            "neighbours       54.40       30.79        22.62     22.62\n",
            "mlpc_classifier  60.32       50.76        41.52     44.39\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Completen con su código acá\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "\n",
        "# Posibles parametros a tocar: Strip accents, lowercase\n",
        "bow_vectorizer = CountVectorizer()  # Vectorizador \"bag of words\"\n",
        "\n",
        "classifiers = [\n",
        "    [\"multinomial\", MultinomialNB()],\n",
        "    [\"decision_tree\", DecisionTreeClassifier(random_state=0)],\n",
        "    [\"neighbours\", KNeighborsClassifier(n_neighbors=10)],\n",
        "    [\"mlpc_classifier\", MLPClassifier(random_state=0, max_iter=100)],\n",
        "]\n",
        "\n",
        "print(\"Bag of Words:\")\n",
        "print(\"Classifier       Accuracy    Precision    Recall    F-Score\")\n",
        "for clf in classifiers:\n",
        "    training_features = bow_vectorizer.fit_transform(\n",
        "        [x[1] for x in train_set]\n",
        "    )  # Se vectorizan los tweets de train\n",
        "\n",
        "    clf[1].fit(\n",
        "        training_features, [x[2] for x in train_set]\n",
        "    )  # Se entrena el clasificador usando los tweets vectorizados\n",
        "\n",
        "    dev_features = bow_vectorizer.transform(\n",
        "        [x[1] for x in dev_set]\n",
        "    )  # Se vectorizan los tweets de dev\n",
        "    prediction = clf[1].predict(\n",
        "        dev_features\n",
        "    )  # Se predicen las categorías de cada tweet (ya vectorizado en la línea anterior)\n",
        "\n",
        "    accuracy = \"{:.2f}\".format(round(accuracy_score([x[2] for x in dev_set], prediction) * 100, 2))\n",
        "    precision = \"{:.2f}\".format(round(precision_score([x[2] for x in dev_set], prediction, average=\"macro\", zero_division=0) * 100, 2))\n",
        "    recall = \"{:.2f}\".format(round(recall_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2))\n",
        "    fscore = \"{:.2f}\".format(round(f1_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2))\n",
        "\n",
        "    print(clf[0] + ' '*(17-len(clf[0])) + accuracy + ' '*7 + precision + ' '*8 + recall + ' '*5 + fscore)  # Se imprime la medida F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuVYSnU7wdKA"
      },
      "source": [
        "Con los F-score obtenidos para los distintos clasificadores, entendemos que para los parámetros dados el clasificador MultiLayerPerception supera a los demás en clasificar el dev_set correctamente utilizando la vectorización Bag of Words. Analizando también los valores de Accuracy, Precision y Recall, entendemos que el MultinomialNB fue más acertivo en encontrar verdaderos positivos, pero resulta en un Recall menor al de MLP por obtener más falsos negativos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74o3beG-_tCq"
      },
      "source": [
        "## Parte 2.2 - TF-iDF\n",
        "\n",
        "El segundo es utilizando TF-iDF (Term Frequency - inverse Document Frequency) para representar los textos.\n",
        "\n",
        "🧐**¿Qué tienen que hacer?**🤔 Lo análogo a la parte anterior pero probando diferentes configuraciones con [TF-iDF de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Comparen los resultados de $F_1$ con los obtenidos para Bag of Words y escriban las conclusiones en una celda de texto a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vgTQJXL_D42e",
        "outputId": "6e1c3052-910a-4e19-b376-19114daf5123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F-Score multinomial: 14.11\n",
            "F-Score decision_tree: 38.69\n",
            "F-Score neighbours: 38.99\n",
            "F-Score mlpc_classifier: 44.15\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Posibles parametros a tocar: Strip accents, lowercase\n",
        "tfid_vectorizer = TfidfVectorizer()\n",
        "  # Vectorizador \"Term Frequency - inverse Document Frequency\"\n",
        "\n",
        "classifiers = [\n",
        "    [\"multinomial\", MultinomialNB()],\n",
        "    [\"decision_tree\", DecisionTreeClassifier(random_state=0)],\n",
        "    [\"neighbours\", KNeighborsClassifier(n_neighbors=10)],\n",
        "    [\"mlpc_classifier\", MLPClassifier(random_state=0, max_iter=100)],\n",
        "]\n",
        "\n",
        "for clf in classifiers:\n",
        "    training_features = tfid_vectorizer.fit_transform(\n",
        "        [x[1] for x in train_set]\n",
        "    )  # Se vectorizan los tweets de train\n",
        "    clf[1].fit(\n",
        "        training_features, [x[2] for x in train_set]\n",
        "    )  # Se entrena el clasificador usando los tweets vectorizados\n",
        "\n",
        "    dev_features = tfid_vectorizer.transform(\n",
        "        [x[1] for x in dev_set]\n",
        "    )  # Se vectorizan los tweets de dev\n",
        "    prediction = clf[1].predict(\n",
        "        dev_features\n",
        "    )  # Se predicen las categorías de cada tweet (ya vectorizado en la línea anterior)\n",
        "\n",
        "    print(\n",
        "        f\"F-Score {clf[0]}: \"\n",
        "        + str(\n",
        "            round(\n",
        "                f1_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2\n",
        "            )\n",
        "        )\n",
        "    )  # Se imprime la medida F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIPSS06GDV-J"
      },
      "source": [
        "## Parte 2.3 - Word embeddings\n",
        "\n",
        "El tercer y último enfoque es utilizando **vectores de palabras estáticos** para representar los textos. Hay muchísimas colecciones de vectores de palabras, pero en esta ocasión vamos a usar unos entrenados por la Univerisdad de Chile. \n",
        "\n",
        "Una idea simple pero útil para representar un tweet puede ser hallar el centroide de los vectores relacionados a las palabras que aparecen en él, y luego comparar cuál es más similar a cuál. \n",
        "\n",
        "🧐**¿Qué tienen que hacer?**🤔 Lo análogo a las partes anteriores pero probando con una representación basada en *embeddings*. Comparen con $F_1$, saquen conclusiones y escríbanlas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIg11w8rGCAN"
      },
      "source": [
        "\n",
        "Les dejamos el siguiente código de ejemplo. Permite cargar los vectores, hallar el centroide de una lista de tokens y calcular las similitudes entre diferentes centroides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se descargan los vectores\n",
        "# !wget -q http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.100k.vec.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qK0kyukPdGXH",
        "outputId": "4ec3c8d3-30f8-4e40-e8c8-c1542bd50ea9"
      },
      "outputs": [],
      "source": [
        "# !gzip -d -q fasttext-sbwc.100k.vec.gz\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6MlhUYqidBvR",
        "outputId": "2c1a18c0-039d-4c72-ba7d-da4aa81ff8c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages/gensim/matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
            "  from scipy.linalg.special_matrices import triu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.0000001]]\n",
            "[[0.90079546]]\n",
            "[[0.78608865]]\n",
            "[[0.7549316]]\n",
            "[[0.5619315]]\n",
            "[[0.70001805]]\n"
          ]
        }
      ],
      "source": [
        "from numpy.linalg import norm\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Se crea el objeto\n",
        "vectors = KeyedVectors.load_word2vec_format('fasttext-sbwc.100k.vec')\n",
        "\n",
        "# Unos ejemplos, ya tokenizados\n",
        "example_1 = [\"Qué\", \"tremendo\", \"día\", \"hace\"]\n",
        "example_2 = [\"Qué\", \"lindo\", \"día\", \"hace\"]\n",
        "example_3 = [\"Hace\", \"un\", \"precioso\", \"día\"]\n",
        "example_4 = [\"Hoy\", \"está\", \"lindo\", \"el\", \"día\"]\n",
        "example_5 = [\"Pah\", \"esta\", \"milanesa\", \"con\", \"mayonesa\", \"está\", \"buenísima\"]\n",
        "example_6 = [\"Ya\", \"le\", \"dije\", \"que\", \"le\", \"vas\", \"a\", \"escribir\"]\n",
        "\n",
        "# Se calculan los centroides\n",
        "centroid_example_1 = np.mean([vectors[word.lower()] for word in example_1], axis=0)\n",
        "centroid_example_2 = np.mean([vectors[word.lower()] for word in example_2], axis=0)\n",
        "centroid_example_3 = np.mean([vectors[word.lower()] for word in example_3], axis=0)\n",
        "centroid_example_4 = np.mean([vectors[word.lower()] for word in example_4], axis=0)\n",
        "centroid_example_5 = np.mean([vectors[word.lower()] for word in example_5], axis=0)\n",
        "centroid_example_6 = np.mean([vectors[word.lower()] for word in example_6], axis=0)\n",
        "\n",
        "# Se imprime la similitud entre los centroides del ejemplo 1 y el resto. \n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_1]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_2]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_3]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_4]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_5]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xS-TBgmIEU3R",
        "outputId": "2c4e00b6-b09c-49e8-f188-93b4ec8bb365"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.33596590774266616"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.utils import tokenize\n",
        "from statistics import mode\n",
        "\n",
        "# Se calculan todos los centroides de test y dev\n",
        "def get_centroid(vectors, tweet):\n",
        "    tokenized_tweet = list(tokenize(tweet, lowercase=True))\n",
        "    v = []\n",
        "    for word in tokenized_tweet:\n",
        "      try:\n",
        "        v.append(vectors[word])\n",
        "      except KeyError:\n",
        "        pass\n",
        "    return np.mean(v, axis=0)\n",
        "\n",
        "\n",
        "train_centroids = np.array([get_centroid(vectors, x[1]) for x in train_set], dtype=float)\n",
        "\n",
        "dev_centroids = np.array([get_centroid(vectors, x[1]) for x in dev_set], dtype=float)\n",
        "\n",
        "classifications = []\n",
        "# Se calcula la similitud de cada centroide de dev a todos los centroides de test\n",
        "dev_similarities = cosine_similarity(dev_centroids, train_centroids)\n",
        "# Se consiguen los 10 indices mas similares\n",
        "ind_most_similar = [np.argpartition(dev_similarity, -10)[-10:] for dev_similarity in dev_similarities]\n",
        "# Se elige la emocion mas frecuente como clasificación\n",
        "classifications = [(mode([train_set[i][2] for i in index])) for index in ind_most_similar]\n",
        "\n",
        "F1 = f1_score([x[2] for x in dev_set], classifications, average=\"macro\")\n",
        "\n",
        "F1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0uSp9-JH4EN"
      },
      "source": [
        "# Ejercicio 3 (opcional)\n",
        "\n",
        "Esta última parte es **opcional**. Ahora que vieron cómo crear clasificadores, invitamos a que intenten construir el mejor clasificador posible utilizando estos enfoques o cualquier otro. Pueden probar lo que quieran, desde enfoques por reglas, utilizando POS-tagging, análisis sintáctico, análisis morfológico o listas de palabras, a modelos neuronales como BERT.\n",
        "\n",
        "Si realizan esta parte opcional, tendrán que entregar en EVA las predicciones para un archivo de *test* que subiremos próximo a la entrega. Los grupos que obtengan las 3 mejores medidas al evaluar en el conjunto de test ganarán 5 puntos porcentuales que sumarán para la nota final del curso.\n",
        "\n",
        "🧐**¿Qué tienen que hacer?**🤔 Construir el mejor clasificador posible y subir a EVA las predicciones para *test*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bi2k9V9FD63d",
        "outputId": "36d52a42-39e0-4f44-c996-b92848f185c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n  Completen con su código acá\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Completen con su código acá\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
