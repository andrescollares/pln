{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4azOYi8KSoC"
      },
      "source": [
        "# Laboratorio de Introducci√≥n al Procesamiento de Lenguaje Natural\n",
        "\n",
        "### Contexto\n",
        "\n",
        "El objetivo de este laboratorio es introducirlos a la construcci√≥n de clasificadores, probando y comparando diferentes m√©todos.\n",
        "\n",
        "### Entrega\n",
        "Lo que deber√°n entregar es un archivo *.ipynb* con su soluci√≥n, que incluya c√≥digo, discusiones y conclusiones del trabajo. \n",
        "\n",
        "‚ö†Ô∏è Es importante que en el archivo a entregar est√©n **las salidas de cada celda ya ejecutadas** ‚ö†Ô∏è. \n",
        "\n",
        "En caso de hacer el ejercicio 3, opcional, deber√°n entregar tambi√©n un archivo .csv **correctamente formateado** con las predicciones de sus modelos.\n",
        "\n",
        "El plazo m√°ximo es el **21 de octubre a las 23:59 horas.**\n",
        "\n",
        "### Plataforma sugerida\n",
        "Sugerimos que utilicen la plataforma [Google colab](https://colab.research.google.com/), que permite trabajar colaborativamente con un *notebook* de python. Al finalizar pueden descargar ese *notebook* en un archivo .ipynb, incluyendo las salidas ya ejecutadas, con la opci√≥n ```File -> Download -> Download .ipynb```\n",
        "\n",
        "### Instalaci√≥n de bibliotecas\n",
        "Antes de empezar, ejecuten esta celda para instalar las dependencias üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzvAUVqGdGW7",
        "outputId": "d81fa012-ae81-4595-d98a-1f72bae4288f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/andres/fing/nsql/lab2/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7FpsJWGszpa",
        "outputId": "df26db7a-346a-4a4c-94ea-dede165cf3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from sklearn) (1.1.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.22.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.9.2)\n",
            "Requirement already satisfied: gensim in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (4.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (1.22.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: spacy in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (3.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
            "Requirement already satisfied: jinja2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.22.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (8.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
            "Requirement already satisfied: nltk in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (3.7)\n",
            "Requirement already satisfied: click in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/andres/fing/nsql/lab2/.venv/lib/python3.10/site-packages (from nltk) (2022.9.13)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install sklearn\n",
        "!python -m pip install gensim\n",
        "!python -m pip install spacy\n",
        "!python -m pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAumcYFLP0f8"
      },
      "source": [
        "# Ejercicio 1 - Primer contacto con el corpus\n",
        "\n",
        "Lo primero a hacer es cargar el corpus. Hay muchas formas de hacerlo ([por ejemplo con Pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)), pero la m√°s sencilla es utilizando funcionalidades nativas de python. El resultado ser√° una lista de n-uplas, donde cada una de ellas se correpondes a una fila del .csv (incluso el cabezal, la primera l√≠nea).\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î \n",
        "Carguen a Colab los archivos necesarios del corpus usando el panel de la izquierda y luego ejecuten las siguientes celdas. Ajusten lo necesario para cargar todo el conjunto de *train* (`train_1.csv` y `train_2.csv`), el de dev y tambi√©n sus anotaciones, que van a ser un subconjunto del archivo `train_1.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nnBJGtH5QLcA"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "\"\"\"\n",
        "  Completen con su c√≥digo de carga de archivos \"train\" y \"dev\" ac√°\n",
        "\"\"\"\n",
        "\n",
        "with open('tweets_grupo_D_1.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  original_annotations = [x for x in reader]\n",
        "\n",
        "with open('train_1.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  train_set = [x for x in reader]\n",
        "\n",
        "with open('train_2.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  train_set = train_set + [x for x in reader] \n",
        "\n",
        "with open('dev.csv', newline='') as corpus_csv:\n",
        "  reader = csv.reader(corpus_csv)\n",
        "  next(reader) # Saltea el cabezal del archivo\n",
        "  dev_set = [x for x in reader]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5vw6mqiNbgD"
      },
      "source": [
        "Imprimamos alg√∫n tweet aleatorio a ver si se carg√≥ bien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ldfWPEtCNebS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e51865b-83f1-4765-bb42-10b6186037ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El tweet es: Y bueno... #LaLiga tiene due√±o üéâüéäüéâüéä #Barcelona #Bar√ßa #BarcaLevante Enhorabuena USER Gracias por tantas emociones, siempre üîµüî¥üîµüî¥üîµüî¥üîµüî¥\n",
            "y su categor√≠a: joy\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Elegir un tweet aleatorio e imprimirlo junto a su categor√≠a\n",
        "random_tweet = random.choice(dev_set)\n",
        "print(f\"El tweet es: {random_tweet[1]}\")\n",
        "print(f\"y su categor√≠a: {random_tweet[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HFf4BwD36uZ"
      },
      "source": [
        "## Parte 1.1 - Composici√≥n de los conjuntos de entrenamiento y desarrollo\n",
        "\n",
        "Para ver c√≥mo esta compuesto el corpus, van a hacer una recorrida sobre todos los tweets en √©l y contar cu√°ntos ejemplos hay de cada categor√≠a. Examinen, discutan y comparen la cantidad de ejemplos en cada categor√≠a, en *train* y en *dev* ¬øhay m√°s ejemplos de unas categor√≠as que de otras? ¬øtienen la misma proporci√≥n en *train* y *dev*?\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Recorran los conjuntos, saquen conclusiones y escr√≠banlas en una celda de texto, a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RNBy_qybD_oY",
        "outputId": "a3335c91-de12-4e2b-eee6-320d0d41fb41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set length: 5723\n",
            "Dev set length: 1250\n",
            "Train set tweet amount by category:\n",
            "{   'anger': 563,\n",
            "    'disgust': 103,\n",
            "    'fear': 63,\n",
            "    'joy': 1230,\n",
            "    'others': 2786,\n",
            "    'sadness': 733,\n",
            "    'surprise': 245}\n",
            "Dev set tweet amount by category:\n",
            "{   'anger': 150,\n",
            "    'disgust': 27,\n",
            "    'fear': 14,\n",
            "    'joy': 259,\n",
            "    'others': 617,\n",
            "    'sadness': 131,\n",
            "    'surprise': 52}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "print(f\"Train set length: {len(train_set)}\")\n",
        "print(f\"Dev set length: {len(dev_set)}\")\n",
        "\n",
        "# Contar cu√°ntos ejemplos hay de cada catergor√≠a\n",
        "# Train\n",
        "train_set_categories = [tweet[2] for tweet in train_set]\n",
        "\n",
        "category_names = set(train_set_categories)\n",
        "\n",
        "train_set_categories_count = dict.fromkeys(category_names, 0)\n",
        "\n",
        "for category in train_set_categories:\n",
        "    train_set_categories_count[category] += 1\n",
        "\n",
        "print(\"Train set tweet amount by category:\")\n",
        "pp.pprint(train_set_categories_count)\n",
        "\n",
        "# Dev\n",
        "dev_set_categories = [tweet[2] for tweet in dev_set]\n",
        "\n",
        "category_names = set(dev_set_categories)\n",
        "\n",
        "dev_set_categories_count = dict.fromkeys(category_names, 0)\n",
        "\n",
        "for category in dev_set_categories:\n",
        "    dev_set_categories_count[category] += 1\n",
        "\n",
        "print(\"Dev set tweet amount by category:\")\n",
        "pp.pprint(dev_set_categories_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDuZTO6udGXD"
      },
      "source": [
        "Aunque hay mas ejemplos de una que de otras, esto es esperable dado que el tama√±o del conjunto de \"\"\"**Dev**\"\"\" (no se como se llama en realidad) es m√°s peque√±o que el de train. Lo importante es que la distribuci√≥n de los datos sea similar y como se puede observar en el resultado de la celda de c√≥digo, esto se cumple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SPrpyYhcdGXD",
        "outputId": "012a4468-e5e2-4234-edc7-e472dc0b1811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set distribution\n",
            "{   'anger': 0.1,\n",
            "    'disgust': 0.02,\n",
            "    'fear': 0.01,\n",
            "    'joy': 0.21,\n",
            "    'others': 0.49,\n",
            "    'sadness': 0.13,\n",
            "    'surprise': 0.04}\n",
            "Dev set distribution\n",
            "{   'anger': 0.12,\n",
            "    'disgust': 0.02,\n",
            "    'fear': 0.01,\n",
            "    'joy': 0.21,\n",
            "    'others': 0.49,\n",
            "    'sadness': 0.1,\n",
            "    'surprise': 0.04}\n"
          ]
        }
      ],
      "source": [
        "print(\"Train set distribution\")\n",
        "pp.pprint({item[0]:round(item[1]/len(train_set),2) for item in train_set_categories_count.items()})\n",
        "print(\"Dev set distribution\")\n",
        "pp.pprint({item[0]:round(item[1]/len(dev_set),2) for item in dev_set_categories_count.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVPMAnvaJkXA"
      },
      "source": [
        "## Parte 1.2 - C√°lculo del acuerdo entre anotadores\n",
        "\n",
        "A continuaci√≥n queremos ver cu√°n de acuerdo estuvieron grupalmente con las anotaciones originales. Para eso deber√°n usar [esta funci√≥n disponible en sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score).\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î  Calculen el grado de acuerdo entre las antoaciones originales y las suyas como grupo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIwwIOilECnH",
        "outputId": "55c2da61-1e49-404d-f054-d49be9ad8937"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.401386263390044"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "original_annotations_categories = [annotation[2] for annotation in original_annotations]\n",
        "train_set_categories = [annotation[2] for annotation in train_set[600:800]]\n",
        "\n",
        "cohen_kappa_score(original_annotations_categories, train_set_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJfW75x5dGXE"
      },
      "source": [
        "TODO: Explicar un poco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRniq2WASeTt"
      },
      "source": [
        "# Ejercicio 2 - Experimentos con clasificadores\n",
        "\n",
        "Ahora que cargaron y examinaron los datos, van a crear un primer clasificador para resolver el problema autom√°ticamente. Como los clasificadores asumen que sus atributos son num√©ricos, hay que encontrar primero una forma num√©rica de representar los textos. En este ejercicio van a experimentar con varias formas de hacer eso.\n",
        "\n",
        "En todas las partes podr√°n usar cualquiera de los clasificadores disponibles en el cat√°logo de modelos de [aprendizaje supervisado de sklearn](https://scikit-learn.org/stable/supervised_learning.html).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZMlbsw2uFLm"
      },
      "source": [
        "## Parte 2.1 - Bag of Words\n",
        "\n",
        "El primer experimento es utilizando Bag of Words (BoW) para representar los textos. Ac√° les dejamos un ejemplo, pero prueben con las diferentes configuraciones que admite [CountVectorizer de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) y con los modelos que quieran del [cat√°logo de sklearn](https://scikit-learn.org/stable/supervised_learning.html). Tambi√©n pueden explorar diferentes formas de limpieza de los textos. \n",
        "\n",
        "Midan el aprendizaje sobre *dev* con la m√©trica [$F_1$, con la implementaci√≥n de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). Tambi√©n pueden usar otras m√©tricas adicionales; queda a su disposici√≥n.\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Hagan varios experimentos con diferentes tipos de clasificadores y diferentes configuraciones de BoW para vectorizar. Midan el aprendizajen con $F_1$. Discutan, reflexionen y escriban las conclusiones en una celda de texto a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G9BNltLduHqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bb54aa-515e-4db0-e164-38b63214b263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            "Classifier       Accuracy    Precision    Recall    F-Score\n",
            "multinomial      61.44       52.35        30.00     30.30\n",
            "decision_tree    54.96       39.90        37.40     38.41\n",
            "neighbours       54.40       30.79        22.62     22.62\n",
            "mlpc_classifier  60.32       50.76        41.52     44.39\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Completen con su c√≥digo ac√°\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "\n",
        "# Posibles parametros a tocar: Strip accents, lowercase\n",
        "bow_vectorizer = CountVectorizer()  # Vectorizador \"bag of words\"\n",
        "\n",
        "classifiers = [\n",
        "    [\"multinomial\", MultinomialNB()],\n",
        "    [\"decision_tree\", DecisionTreeClassifier(random_state=0)],\n",
        "    [\"neighbours\", KNeighborsClassifier(n_neighbors=10)],\n",
        "    [\"mlpc_classifier\", MLPClassifier(random_state=0, max_iter=100)],\n",
        "]\n",
        "\n",
        "print(\"Bag of Words:\")\n",
        "print(\"Classifier       Accuracy    Precision    Recall    F-Score\")\n",
        "for clf in classifiers:\n",
        "    training_features = bow_vectorizer.fit_transform(\n",
        "        [x[1] for x in train_set]\n",
        "    )  # Se vectorizan los tweets de train\n",
        "\n",
        "    clf[1].fit(\n",
        "        training_features, [x[2] for x in train_set]\n",
        "    )  # Se entrena el clasificador usando los tweets vectorizados\n",
        "\n",
        "    dev_features = bow_vectorizer.transform(\n",
        "        [x[1] for x in dev_set]\n",
        "    )  # Se vectorizan los tweets de dev\n",
        "    prediction = clf[1].predict(\n",
        "        dev_features\n",
        "    )  # Se predicen las categor√≠as de cada tweet (ya vectorizado en la l√≠nea anterior)\n",
        "\n",
        "    accuracy = \"{:.2f}\".format(round(accuracy_score([x[2] for x in dev_set], prediction) * 100, 2))\n",
        "    precision = \"{:.2f}\".format(round(precision_score([x[2] for x in dev_set], prediction, average=\"macro\", zero_division=0) * 100, 2))\n",
        "    recall = \"{:.2f}\".format(round(recall_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2))\n",
        "    fscore = \"{:.2f}\".format(round(f1_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2))\n",
        "\n",
        "    print(clf[0] + ' '*(17-len(clf[0])) + accuracy + ' '*7 + precision + ' '*8 + recall + ' '*5 + fscore)  # Se imprime la medida F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con los F-score obtenidos para los distintos clasificadores, entendemos que para los par√°metros dados el clasificador MultiLayerPerception supera a los dem√°s en clasificar el dev_set correctamente utilizando la vectorizaci√≥n Bag of Words. Analizando tambi√©n los valores de Accuracy, Precision y Recall, entendemos que el MultinomialNB fue m√°s acertivo en encontrar verdaderos positivos, pero resulta en un Recall menor al de MLP por obtener m√°s falsos negativos."
      ],
      "metadata": {
        "id": "TuVYSnU7wdKA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74o3beG-_tCq"
      },
      "source": [
        "## Parte 2.2 - TF-iDF\n",
        "\n",
        "El segundo es utilizando TF-iDF (Term Frequency - inverse Document Frequency) para representar los textos.\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Lo an√°logo a la parte anterior pero probando diferentes configuraciones con [TF-iDF de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Comparen los resultados de $F_1$ con los obtenidos para Bag of Words y escriban las conclusiones en una celda de texto a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgTQJXL_D42e",
        "outputId": "6e1c3052-910a-4e19-b376-19114daf5123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F-Score multinomial: 14.11\n",
            "F-Score decision_tree: 38.69\n",
            "F-Score neighbours: 38.99\n",
            "F-Score mlpc_classifier: 44.15\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Posibles parametros a tocar: Strip accents, lowercase\n",
        "tfid_vectorizer = TfidfVectorizer()\n",
        "  # Vectorizador \"Term Frequency - inverse Document Frequency\"\n",
        "\n",
        "classifiers = [\n",
        "    [\"multinomial\", MultinomialNB()],\n",
        "    [\"decision_tree\", DecisionTreeClassifier(random_state=0)],\n",
        "    [\"neighbours\", KNeighborsClassifier(n_neighbors=10)],\n",
        "    [\"mlpc_classifier\", MLPClassifier(random_state=0, max_iter=100)],\n",
        "]\n",
        "\n",
        "for clf in classifiers:\n",
        "    training_features = tfid_vectorizer.fit_transform(\n",
        "        [x[1] for x in train_set]\n",
        "    )  # Se vectorizan los tweets de train\n",
        "    clf[1].fit(\n",
        "        training_features, [x[2] for x in train_set]\n",
        "    )  # Se entrena el clasificador usando los tweets vectorizados\n",
        "\n",
        "    dev_features = tfid_vectorizer.transform(\n",
        "        [x[1] for x in dev_set]\n",
        "    )  # Se vectorizan los tweets de dev\n",
        "    prediction = clf[1].predict(\n",
        "        dev_features\n",
        "    )  # Se predicen las categor√≠as de cada tweet (ya vectorizado en la l√≠nea anterior)\n",
        "\n",
        "    print(\n",
        "        f\"F-Score {clf[0]}: \"\n",
        "        + str(\n",
        "            round(\n",
        "                f1_score([x[2] for x in dev_set], prediction, average=\"macro\") * 100, 2\n",
        "            )\n",
        "        )\n",
        "    )  # Se imprime la medida F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIPSS06GDV-J"
      },
      "source": [
        "## Parte 2.3 - Word embeddings\n",
        "\n",
        "El tercer y √∫ltimo enfoque es utilizando **vectores de palabras est√°ticos** para representar los textos. Hay much√≠simas colecciones de vectores de palabras, pero en esta ocasi√≥n vamos a usar unos entrenados por la Univerisdad de Chile. \n",
        "\n",
        "Una idea simple pero √∫til para representar un tweet puede ser hallar el centroide de los vectores relacionados a las palabras que aparecen en √©l, y luego comparar cu√°l es m√°s similar a cu√°l. \n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Lo an√°logo a las partes anteriores pero probando con una representaci√≥n basada en *embeddings*. Comparen con $F_1$, saquen conclusiones y escr√≠banlas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIg11w8rGCAN"
      },
      "source": [
        "\n",
        "Les dejamos el siguiente c√≥digo de ejemplo. Permite cargar los vectores, hallar el centroide de una lista de tokens y calcular las similitudes entre diferentes centroides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK0kyukPdGXH",
        "outputId": "4ec3c8d3-30f8-4e40-e8c8-c1542bd50ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev.csv                 Letra_laboratorio.ipynb  train_2.csv\n",
            "fasttext-sbwc.100k.vec  train_1.csv              tweets_grupo_D_1.csv\n"
          ]
        }
      ],
      "source": [
        "# Se descargan los vectores\n",
        "!wget -q http://dcc.uchile.cl/~jperez/word-embeddings/fasttext-sbwc.100k.vec.gz\n",
        "!gzip -d -q fasttext-sbwc.100k.vec.gz\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MlhUYqidBvR",
        "outputId": "2c1a18c0-039d-4c72-ba7d-da4aa81ff8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 2.56507471e-02 -2.89550006e-01 -1.70209005e-01 -1.13342851e-01\n",
            "  2.06646740e-01  4.34514955e-02 -2.02471748e-01 -3.34815197e-02\n",
            "  4.63898294e-02  1.33737102e-01 -1.95077509e-02 -2.50672754e-02\n",
            "  2.28695013e-02  1.34787261e-01  2.73027495e-02  8.70652497e-02\n",
            "  1.17836520e-01  2.25424245e-01 -1.06735025e-02  3.15292478e-02\n",
            " -1.39987499e-01 -8.80450010e-02 -5.78614995e-02  1.26611248e-01\n",
            "  2.04597488e-02 -1.91182494e-01  1.32874250e-01  4.12722528e-02\n",
            " -1.26742497e-01 -1.59242004e-01 -6.53272495e-03  2.06380002e-02\n",
            " -1.52943254e-01  6.02030009e-02 -5.00517488e-02 -3.97852510e-02\n",
            " -2.23585010e-01 -1.39164999e-01  1.64078012e-01  9.42077488e-02\n",
            "  7.43749961e-02 -6.84305280e-02 -1.79119986e-02 -4.92537506e-02\n",
            " -1.61272511e-02  3.31189990e-01 -9.34137329e-02 -2.53442496e-01\n",
            " -9.29175019e-02  9.02502537e-02  2.39030495e-01  2.91985013e-02\n",
            "  1.21482752e-01 -1.52302496e-02 -6.85224682e-03 -3.89675014e-02\n",
            " -6.83792531e-02 -2.81797498e-01  2.65489995e-01  4.67660017e-02\n",
            " -1.09462500e-01  1.75572559e-03  1.85552508e-01  1.42499804e-03\n",
            "  5.04327491e-02  7.02075064e-02 -2.87537519e-02  5.81612475e-02\n",
            " -7.47646540e-02 -3.87869999e-02  2.57286504e-02 -6.25844970e-02\n",
            " -1.40414238e-01 -5.04442491e-02  1.70948997e-01 -4.63907495e-02\n",
            "  1.20194897e-01  9.59082544e-02 -1.63562223e-01 -2.42391005e-01\n",
            "  2.30722487e-01  1.51555717e-01  7.36175030e-02  9.48175043e-02\n",
            " -2.41674893e-02  4.87282500e-02  1.40295506e-01 -5.21330014e-02\n",
            " -5.05847521e-02 -1.32644743e-01 -8.50242525e-02 -6.33020028e-02\n",
            " -8.61774981e-02 -1.33791253e-01  2.39777490e-01 -1.14177495e-01\n",
            " -4.42297496e-02 -1.26282498e-01  9.57470015e-02  3.67355011e-02\n",
            " -5.45067489e-02 -5.81250526e-04 -9.72587466e-02  1.39142245e-01\n",
            "  5.00841290e-02  7.65883997e-02 -9.30235088e-02  1.23274997e-01\n",
            "  1.03682503e-01 -1.14310496e-01  1.13725260e-01  6.15890026e-02\n",
            " -1.53298199e-01  1.44571006e-01 -6.28172532e-02  3.91385034e-02\n",
            " -1.79032996e-01  1.91782489e-01 -2.25018524e-02 -1.37161255e-01\n",
            " -1.21959656e-01 -5.54749928e-03  1.46937504e-01  1.10261753e-01\n",
            " -8.27047527e-02 -1.00002497e-01  2.97484994e-02 -1.12730991e-02\n",
            " -2.10441247e-01 -9.08717513e-03  1.01370020e-02  1.37657508e-01\n",
            "  1.51875019e-02 -3.07047516e-02  2.32840002e-01 -1.40600000e-02\n",
            "  2.22137459e-02  6.67380691e-02 -1.01974998e-02 -7.44775012e-02\n",
            "  6.05137497e-02 -1.04459502e-01 -2.89909989e-01  3.50162536e-02\n",
            "  8.35397542e-02  2.99022477e-02 -1.36871487e-01 -8.37500021e-03\n",
            " -1.99598745e-01  8.06879997e-02  1.51158750e-01  2.02971488e-01\n",
            " -6.85878247e-02  1.06747746e-01 -7.30319023e-02  1.03964500e-01\n",
            " -1.07410759e-01  1.72584951e-02 -2.38422472e-02 -9.59317535e-02\n",
            " -6.85539991e-02 -7.64507428e-02 -1.57129988e-02  8.59300047e-02\n",
            "  2.11821496e-01 -2.35562503e-01  3.09580028e-01 -8.21969956e-02\n",
            "  1.04889244e-01  1.79932490e-02 -1.45476758e-01 -1.30148008e-01\n",
            "  1.47028267e-02 -1.95965022e-02 -1.42650027e-02 -9.40877497e-02\n",
            " -6.07174821e-03  6.47090003e-02 -8.32663476e-02 -1.02754705e-01\n",
            " -1.90770000e-01 -4.16799895e-02  7.93990046e-02  4.10097502e-02\n",
            "  1.35757998e-01  1.56659991e-01  4.74947467e-02 -1.30204991e-01\n",
            " -1.74343497e-01  1.07636750e-01 -1.29724994e-01 -7.71017522e-02\n",
            "  1.23894997e-01  9.91024971e-02  6.63082525e-02 -4.11083996e-02\n",
            "  1.90374509e-01 -1.80282742e-01 -1.60590000e-02  1.24228552e-01\n",
            "  1.33745238e-01  9.42167491e-02 -1.54760987e-01  3.61520275e-02\n",
            " -6.22249991e-02 -1.47601247e-01 -2.30100259e-01  3.21375113e-03\n",
            "  2.47425009e-02 -3.68599966e-02  1.67278841e-01  1.22402515e-02\n",
            "  1.61227509e-01 -5.68475015e-02 -1.32371500e-01 -1.31284148e-01\n",
            " -1.48999505e-04  7.64225051e-02  1.92212507e-01 -2.14057490e-01\n",
            " -8.97125080e-02 -3.09079997e-02  7.70414993e-02 -6.37724996e-03\n",
            "  7.88974985e-02 -2.07526267e-01 -2.00504243e-01 -2.36992508e-01\n",
            "  7.79297873e-02 -1.79640993e-01 -1.30750053e-02  2.46355042e-01\n",
            "  2.79632509e-01  2.31734999e-02 -8.51042494e-02  1.72247499e-01\n",
            " -1.12022497e-01 -3.20607483e-01 -1.79529972e-02 -5.11630028e-02\n",
            "  1.74090043e-02 -1.02207504e-01 -5.98320439e-02  1.04386501e-01\n",
            "  2.18295500e-01 -1.33462995e-01  8.83152485e-02  1.93076506e-01\n",
            " -1.07025497e-01  1.15355998e-01  2.21350044e-03  1.88617751e-01\n",
            " -3.36407751e-01  4.27932478e-02 -9.37246457e-02 -1.25624985e-03\n",
            " -2.74007261e-01  1.63394749e-01  7.64230043e-02  6.26122504e-02\n",
            " -5.61529994e-02 -2.00495031e-02 -2.78647512e-01  2.40132496e-01\n",
            "  4.09055017e-02  1.45883247e-01 -1.18886493e-01  6.00607507e-02\n",
            " -8.63612443e-02  2.86019266e-01  7.13820010e-02 -1.61329851e-01\n",
            "  3.07892501e-01 -1.01808198e-01 -2.89119989e-01  7.92172477e-02\n",
            "  5.54717481e-02  1.58399016e-01  7.74932504e-02  1.52934760e-01\n",
            " -1.39274582e-01 -1.15615018e-02  6.78434968e-02 -3.88078056e-02\n",
            "  6.84112459e-02  1.06725246e-01  1.42609999e-02  1.43261880e-01\n",
            "  3.28842513e-02  3.03499997e-02  1.67834997e-01  6.49997741e-02\n",
            " -2.12624997e-01 -7.01425001e-02  2.27715001e-02 -1.09204993e-01\n",
            "  9.58832428e-02 -5.98699972e-02  1.78015754e-01 -2.43507504e-01]\n",
            "[[1.0000001]]\n",
            "[[0.90079546]]\n",
            "[[0.78608865]]\n",
            "[[0.7549316]]\n",
            "[[0.5619315]]\n",
            "[[0.70001805]]\n"
          ]
        }
      ],
      "source": [
        "from numpy.linalg import norm\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Se crea el objeto\n",
        "vectors = KeyedVectors.load_word2vec_format('fasttext-sbwc.100k.vec')\n",
        "\n",
        "# Unos ejemplos, ya tokenizados\n",
        "example_1 = [\"Qu√©\", \"tremendo\", \"d√≠a\", \"hace\"]\n",
        "example_2 = [\"Qu√©\", \"lindo\", \"d√≠a\", \"hace\"]\n",
        "example_3 = [\"Hace\", \"un\", \"precioso\", \"d√≠a\"]\n",
        "example_4 = [\"Hoy\", \"est√°\", \"lindo\", \"el\", \"d√≠a\"]\n",
        "example_5 = [\"Pah\", \"esta\", \"milanesa\", \"con\", \"mayonesa\", \"est√°\", \"buen√≠sima\"]\n",
        "example_6 = [\"Ya\", \"le\", \"dije\", \"que\", \"le\", \"vas\", \"a\", \"escribir\"]\n",
        "\n",
        "# Se calculan los centroides\n",
        "centroid_example_1 = np.mean([vectors[word.lower()] for word in example_1], axis=0)\n",
        "centroid_example_2 = np.mean([vectors[word.lower()] for word in example_2], axis=0)\n",
        "centroid_example_3 = np.mean([vectors[word.lower()] for word in example_3], axis=0)\n",
        "centroid_example_4 = np.mean([vectors[word.lower()] for word in example_4], axis=0)\n",
        "centroid_example_5 = np.mean([vectors[word.lower()] for word in example_5], axis=0)\n",
        "centroid_example_6 = np.mean([vectors[word.lower()] for word in example_6], axis=0)\n",
        "\n",
        "# Se imprime la similitud entre los centroides del ejemplo 1 y el resto. \n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_1]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_2]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_3]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_4]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_5]))\n",
        "print(cosine_similarity([centroid_example_1],[centroid_example_6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS-TBgmIEU3R",
        "outputId": "2c4e00b6-b09c-49e8-f188-93b4ec8bb365"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n  Completen con su c√≥digo ac√°\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Completen con su c√≥digo ac√°\n",
        "\"\"\"\n",
        "\n",
        "tweets = [x[1] for x in train_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0uSp9-JH4EN"
      },
      "source": [
        "# Ejercicio 3 (opcional)\n",
        "\n",
        "Esta √∫ltima parte es **opcional**. Ahora que vieron c√≥mo crear clasificadores, invitamos a que intenten construir el mejor clasificador posible utilizando estos enfoques o cualquier otro. Pueden probar lo que quieran, desde enfoques por reglas, utilizando POS-tagging, an√°lisis sint√°ctico, an√°lisis morfol√≥gico o listas de palabras, a modelos neuronales como BERT.\n",
        "\n",
        "Si realizan esta parte opcional, tendr√°n que entregar en EVA las predicciones para un archivo de *test* que subiremos pr√≥ximo a la entrega. Los grupos que obtengan las 3 mejores medidas al evaluar en el conjunto de test ganar√°n 5 puntos porcentuales que sumar√°n para la nota final del curso.\n",
        "\n",
        "üßê**¬øQu√© tienen que hacer?**ü§î Construir el mejor clasificador posible y subir a EVA las predicciones para *test*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi2k9V9FD63d",
        "outputId": "36d52a42-39e0-4f44-c996-b92848f185c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n  Completen con su c√≥digo ac√°\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "  Completen con su c√≥digo ac√°\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}